{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgromann/MCMLR_2024W/blob/main/Bonus_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bonus Exercises 2: Low-Rank Adaptation and Crosslingual Transfer**\n",
        "\n",
        "\n",
        "\n",
        "This notebook represents the second bonus exercises for the lecture Multilingual and Crosslingual Methods and Language Resources (2024W 340168-1). For each successfully completed bonus exercise, a maximum of three points can be achieved that will be added to the points of the final exam. The tasks to be completed in the following notebook are marked with ðŸ‘‹ âš’.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H_RsHNVC57Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use Low-Rank Adaptation to Fine-Tune XLM-R on the task of linguistic acceptability in English and then test its zero-shot capability in other languages.\n",
        "\n",
        "# **Make sure to set your runtime to GPU before you start training.**\n",
        "\n",
        "(Tab: Runtime/Change Runtime Type -> Select GPU)"
      ],
      "metadata": {
        "id": "v5cE7oECGoNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Fine-Tuning on English**\n",
        "\n",
        "The first part has already been prepared for you. We will load and preprocess the Corpus for Linguistic Acceptability (COLA) dataset from GLUE and then use Low-Rank Adaptation to fine-tune XLM-R."
      ],
      "metadata": {
        "id": "lmoyUnpZKmma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation\n",
        "\n",
        "As always, we first need to install the necessary libraries. One that is new in this notebook is the Parameter-Efficient Fine-Tuning (PEFT) library."
      ],
      "metadata": {
        "id": "_RWUW8a4djOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U evaluate\n",
        "!pip install -U datasets\n",
        "!pip install -U transformers\n",
        "!pip install -U peft"
      ],
      "metadata": {
        "id": "IRnus8Ljb1QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "\n",
        "In this notebook we will first be using the COLA dataset from the GLUE library and then a multilingual extension.\n",
        " We will first train on English and transfer to another language and evaluate zero-shot transfer on one more language (see [here](https://huggingface.co/datasets/Geralt-Targaryen/MELA) for a selection)."
      ],
      "metadata": {
        "id": "UT-WD8SmAvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_en = load_dataset(\"glue\", \"cola\")\n",
        "dataset_en.num_rows"
      ],
      "metadata": {
        "id": "snp8ndm5gZEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us take a look at the components of the dataset."
      ],
      "metadata": {
        "id": "9d5HMENBesKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_en['train'].features"
      ],
      "metadata": {
        "id": "R1zcS7FtevB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face Datasets is designed to be interoperable with libraries like Pandas, as well as NumPy, PyTorch, TensorFlow, and JAX. To enable the conversion between various third-party libraries, Hugging Face Datasets provides a Dataset.set_format() function. This function only changes the output format of the dataset, so you can easily switch to another format without affecting the underlying data format which is Apache Arrow. The formatting is done in-place, so letâ€™s convert our dataset to Pandas and look at a random sample:"
      ],
      "metadata": {
        "id": "kuTCNH3Fe61B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "dataset_en.set_format(\"pandas\")\n",
        "df = dataset_en[\"train\"][:]\n",
        "# Create a random sample\n",
        "sample = df.sample(n=5, random_state=42)\n",
        "display(HTML(sample.to_html()))"
      ],
      "metadata": {
        "id": "FN4bCWnce7RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pandas dataframe can now be used as we would always use Pandas, for instance to count the number of labels for `cause` in the column question."
      ],
      "metadata": {
        "id": "kB2qLMNvfhcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "N2pCWD7JfgQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the two labels are spread quite evenly across the two types of questions.\n",
        "\n",
        "This was just a brief detour to show how datasets can be nicely manipulated and displayed using other libraries. We will now get back to our usual datasets library from Hugging Face. To this end, we will reset the format."
      ],
      "metadata": {
        "id": "6dVdd-W7f8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_en.reset_format()"
      ],
      "metadata": {
        "id": "OwPKWfq1hLoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Dataset\n",
        "\n",
        "In this example, we model COPA as a multiple-choice task with two choices. Thus, we encode the premise and question as well as both choices as one input to our `xlm-roberta-base` model. Using `dataset.map()`, we can pass the full dataset through the tokenizer in batches."
      ],
      "metadata": {
        "id": "wCujQTNRLHFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
        "batch_size = 32\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=True, truncation=True)\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "  token_dataset = dataset.map(tokenize_function, batched=True, batch_size=batch_size)\n",
        "  tokenized_dataset = token_dataset.rename_column(\"label\", \"labels\")\n",
        "  return tokenized_dataset\n",
        "\n",
        "data_set_en_with_test = DatasetDict(\n",
        "    train=dataset_en['train'].shuffle(seed=24).select(range(7488)),\n",
        "    validation=dataset_en['validation'],\n",
        "    test=dataset_en['train'].shuffle(seed=24).select(range(7488, 8551)),\n",
        ")\n",
        "\n",
        "tokenized_dataset_en = preprocess_dataset(data_set_en_with_test)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "ei3DaO3mgnX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_en[\"train\"][1]"
      ],
      "metadata": {
        "id": "q7q0udiH0TRa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "## **Low-Rank Adaptation (LoRA)**\n",
        "\n",
        "In order to perform low-rank adaptation (LoRA) on a pretrained language model for parameter-efficient fine-tuning (PEFT), we need to set a few parameters in the LoRA Configuration. Hugging Face offers some [documentation on LoRA](https://huggingface.co/docs/peft/main/en/developer_guides/lora).\n",
        "\n",
        "The `task-type` specifies which task the model should be fine-tuned on and needs to correspond to the way the model is loaded. If we load a model for Sequence Classification, also the task needs to be `SEQ_CLS`, an abbreviation for Sequence Classification. Then the dataset needs to be one with an input sequence and a number of target classes.\n",
        "\n",
        "The `target-module` depends on the type of model, which for XLM-R is `[\"query\", \"value\"]`. Since we wish to change model parameters, the inference mode is set to false. The variable `r`indicates the rank to which the dimensionality is being reduced. The variable `alpha` is a scaling parameter, because `r`scales at 1.0. With small datasets or if unsure, the rank and alpha can be the same. Finally, dropout is a random omission of trainable parameters (setting to zero) during training, mostly to avoid overfitting.\n",
        "\n",
        "Feel free to play with and adapt these parameters if you are interested in seeing the effect.\n"
      ],
      "metadata": {
        "id": "fZ8nhE-4LXNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftType, get_peft_model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "peft_type = PeftType.LORA\n",
        "peft_config = LoraConfig(task_type=\"SEQ_CLS\", target_modules=[\"query\", \"value\"], inference_mode=False, r=8, lora_alpha=8, lora_dropout=0.1)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-large\")\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ],
      "metadata": {
        "id": "mmA3RxaSg3-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "num_train_epochs = 5\n",
        "logging_steps = len(tokenized_dataset_en[\"train\"]) // (batch_size * num_train_epochs)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    report_to='none',\n",
        "    load_best_model_at_end=True,\n",
        "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "    remove_unused_columns=True,\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # calculates the accuracy\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset_en[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_en[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "l-SNWkDnihuK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have configured the model with PEFT, we can train the PEFT model as usual."
      ],
      "metadata": {
        "id": "RG5HncHoRIAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "sGIdhKGgjkIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Evaluate on the English test set to see how well the fine-tuning has worked."
      ],
      "metadata": {
        "id": "LXIkfddAH4fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for the evaluation here\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"/content/training_output/checkpoint-1340\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)\n",
        "\n",
        "model_inputs = tokenizer(\"I ordered if John dink his beer.\", return_tensors=\"pt\")\n",
        "outputs = inference_model(**model_inputs)\n",
        "prediction = outputs.logits.argmax(dim=-1)\n",
        "print(prediction)\n",
        "print([\"unacceptable\", \"acceptable\"][prediction])"
      ],
      "metadata": {
        "id": "B84dxesrJH-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Crosslingual Transfer**\n",
        "\n",
        "In this section, we will be using the Multilingual Evaluation of Linguistic Acceptability ([MELA](https://github.com/sjtu-compling/mela?tab=readme-ov-file)), which is also [available on Hugging Face](https://huggingface.co/datasets/Geralt-Targaryen/MELA) to test the transfer and zero-shot capabilities of XLM-R with LoRA Fine-Tuning.\n",
        "\n",
        "We will first fine-tune on German and then test the on German but also in a zero-shot approach on another language of your choice.\n",
        "\n",
        "Please be aware of the fact that MELA \"only\" offers a dev and a test set - no train, validation, test split. Thus, the preprocessing needs to be slightly adapted."
      ],
      "metadata": {
        "id": "LfVSDWxbGaAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "de = load_dataset(\"Geralt-Targaryen/MELA\", \"de\")\n",
        "dataset_de = preprocess_dataset(de)\n",
        "print(dataset_de[\"test\"][20])"
      ],
      "metadata": {
        "id": "-Ad2i7j49qdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Use the German dev partition to further-finetune the previously configured model and then evaluate on the test partition of the German dataset."
      ],
      "metadata": {
        "id": "rC4WwjbeKGFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning on German dev set here"
      ],
      "metadata": {
        "id": "x0H1RhyqKFG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ âš’ Select another language of your choice from the [MELA dataset](https://huggingface.co/datasets/Geralt-Targaryen/MELA) to only evaluate the fine-tuned model (zero-shot capability).\n",
        "\n",
        "**Alternative**: Feel free to create your own mini-dataset of a few (non)-acceptable sentences in a language of your choice to test the model's zero-shot capacity."
      ],
      "metadata": {
        "id": "iEbyOVU6KqGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your evaluation here"
      ],
      "metadata": {
        "id": "UdYUU4MoK0jE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}